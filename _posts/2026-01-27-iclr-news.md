---
layout: post
title: "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models"
thumbnail-img: /assets/img/ace-logo.png
share-img: /assets/img/ace-logo.png
author: Qizheng Zhang
image: /assets/img/ace-logo.png
---
<br>

<!-- <span style="font-size:1.3em;">**TL;DR:** ACE turboboosts LLM agents with **<span style="color:#2AA198;">7Ã—</span>** faster access to **<span style="color:#2AA198;">100x</span>** more KV caches, for both multi-turn conversation **<span style="color:#2AA198;"> and RAG</span>** </span>.

<br> -->

**[[ðŸ’» Source code]](https://github.com/ace-agent/ace) &nbsp;  [[ðŸ“š Paper]](https://arxiv.org/pdf/2510.04618)**


<!-- LLMs are ubiquitous across industries, but when using them with long documents, *it takes forever for the model even to spit out the first token*. 

Meet **ACE**, a new *open-source* system developed in UChicago that generates the first token 3-10x faster than vLLM by efficiently managing long documents (and other sources of knowledge) for you. -->


<div align="center">
<img src="/assets/img/ace-system.png" alt="Icon" style="width:700px; vertical-align:middle;">
</div>

<br>
<br>

## What is ACE?

<!-- This table contrasts vLLM with vs. without LMCache:

<div align="center">
<img src="/assets/img/lmcache-contrast.png" alt="Icon" style="width:700px; vertical-align:middle;">
</div> -->

At its core, ACE incorporated two recent research projects from Stanford, SambaNova Systems, and UC Berkeley.

- **Agentic Context Engineering [[ICLR 2026]](https://arxiv.org/abs/2510.04618)** ...

- **Agentic Plan Caching [[NeurIPS 2025]](https://arxiv.org/abs/2506.14852)** ...

<br>
<be>

## Contact Us

Shoot us an email at [qizhengz@stanford.edu](mailto:qizhengz@stanford.edu).
