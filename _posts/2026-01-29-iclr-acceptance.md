---
layout: post
title: "Agentic Context Engineering (ACE) accepted at ICLR 2026"
thumbnail-img: /assets/img/iclr-logo.png
share-img: /assets/img/iclr-logo.png
author: Qizheng Zhang
image: /assets/img/iclr-logo.png
---
üéâ **Our paper _Agentic Context Engineering (ACE)_ has been accepted to ICLR 2026!**

We are excited to share that our work on building *self-improving language agents through context evolution* will appear at the International Conference on Learning Representations (ICLR) 2026.

ACE explores a simple but powerful idea: instead of updating model weights, how do we continuously **evolve an agent‚Äôs context, memory, and tools at test time** to make it smarter over time? The ACE paper improves both quality and efficiency for the emerging learning context problem on both offline and online scenarios.

We release the code on [Github](https://github.com/ace-agent/ace) with easy reproduction. Moreover, we are building the ACE repo as a **research platform** for researchers to further explore context-based learning and self-improving agents. We are currently filling the support matrix for the most popular datasets and current frameworks, and welcome contributions from the community!

**[[üíª Source code]](https://github.com/ace-agent/ace) &nbsp;  [[üìö Paper]](https://arxiv.org/pdf/2510.04618)**

<br>

## What is ACE?

Large language model agents are increasingly deployed in complex, multi-step environments: coding assistants, research copilots, data analysis agents, and more. But today‚Äôs systems are often *stateless*: each run starts fresh, discarding what was learned from previous attempts.

**Agentic Context Engineering (ACE)** introduces a new paradigm for *test-time adaptation*:

> Instead of changing weights, ACE evolves the **context** supplied to an agent (its memories, plans, summaries, tools, and distilled experience) so the agent improves as it operates.

<div align="center">
<img src="/assets/img/ace-system.png" alt="Icon" style="width:700px; vertical-align:middle;">
</div>

Concretely, ACE provides:

- **Empirical analysis and new understanding** of why prior context-evolution methods can fail silently or suddenly in multi-turn, long-horizon agentic tasks
- **A unified framework** for capturing and structuring experience from prior executions without suffering from brevity bias and context collapse
- **Evaluation across diverse agentic and reasoning tasks**, showing consistent performance and efficiency gains without retraining the underlying model

Across coding, tool-use, and domain-specific reasoning benchmarks, ACE demonstrates that carefully engineered and continuously evolving contexts can unlock strong improvements in task success rate, efficiency, and robustness, thus pushing toward **self-improving AI systems** that learn from experience in deployment.

If you are interested in how test-time memory, learning from experience, reflection, and planning interact with modern LLM systems, we think ACE will resonate with you! 

<br>

## ACE as a Context Engineering Research Platform

Since releasing ACE, we‚Äôve been thrilled to see growing interest from the research and open-source communities:

- ‚≠êÔ∏è Active development at [github.com/ace-agent/ace](https://github.com/ace-agent/ace)
- üì£ Talks and presentations at Hippocratic AI, HKU NAISS Lab, and more to come
- ü§ù New research collaborations building on ACE as a platform for studying context-based methods and self-evolving agents at Stanford, MIT, ...

Qizheng put a support graph here, include datasets already included, dataset being constructed, (same for frameworks like RLM, blabla)

ICLR 2026 is just the beginning --- we see ACE as part of a broader research agenda around the new context learning paradigm. If you‚Äôre working on related problems or want to build on the ACE platform, we‚Äôd love to hear from you! 

<br>

## Contact Us

Shoot us an email at [qizhengz@stanford.edu](mailto:qizhengz@stanford.edu).